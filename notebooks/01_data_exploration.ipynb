{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44d11b8e",
   "metadata": {},
   "source": [
    "# Data Exploration - GTA Real Estate Hotspots\n",
    "\n",
    "**Author:** Kyle Williamson (Data Engineer)  \n",
    "**Date:** 2024-11-06  \n",
    "**Purpose:** Initial exploration of collected data sources\n",
    "\n",
    "## Objectives\n",
    "1. Load and validate all data sources\n",
    "2. Understand data structure and quality\n",
    "3. Identify missing values and outliers\n",
    "4. Generate summary statistics\n",
    "5. Create initial visualizations\n",
    "\n",
    "## Data Sources\n",
    "- Toronto Open Data: Real estate, building permits, demographics\n",
    "- OpenStreetMap: Road networks, POIs, transit stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bc6f5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Geospatial\n",
    "import geopandas as gpd\n",
    "import folium\n",
    "from folium import plugins\n",
    "\n",
    "# Network analysis\n",
    "import networkx as nx\n",
    "\n",
    "# Add src to path\n",
    "PROJECT_ROOT = Path.cwd().parent if Path.cwd().name == 'notebooks' else Path.cwd()\n",
    "sys.path.append(str(PROJECT_ROOT / 'src'))\n",
    "\n",
    "# Set paths\n",
    "DATA_RAW = PROJECT_ROOT / 'data' / 'raw'\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "RESULTS = PROJECT_ROOT / 'results'\n",
    "\n",
    "# Create results directories\n",
    "(RESULTS / 'figures').mkdir(parents=True, exist_ok=True)\n",
    "(RESULTS / 'tables').mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Imports successful\")\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Data directory: {DATA_RAW}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9365a56c",
   "metadata": {},
   "source": [
    "## 1. Real Estate Data Exploration\n",
    "\n",
    "Let's start by loading and exploring the real estate/housing data from Toronto Open Data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc037f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent real estate file\n",
    "real_estate_dir = DATA_RAW / 'real_estate'\n",
    "\n",
    "if not real_estate_dir.exists():\n",
    "    print(\"Real estate data not found. Run data collection first:\")\n",
    "    print(\"   python src/data_collection.py --sources real_estate\")\n",
    "else:\n",
    "    # Get most recent file\n",
    "    csv_files = list(real_estate_dir.glob('*.csv'))\n",
    "    \n",
    "    if csv_files:\n",
    "        latest_file = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
    "        print(f\"âœ“ Loading: {latest_file.name}\")\n",
    "        \n",
    "        df_real_estate = pd.read_csv(latest_file)\n",
    "        \n",
    "        print(f\"\\n Dataset Shape: {df_real_estate.shape}\")\n",
    "        print(f\"   Rows: {df_real_estate.shape[0]:,}\")\n",
    "        print(f\"   Columns: {df_real_estate.shape[1]}\")\n",
    "        \n",
    "        print(f\"\\n Columns:\")\n",
    "        for i, col in enumerate(df_real_estate.columns, 1):\n",
    "            print(f\"   {i:2d}. {col}\")\n",
    "        \n",
    "        print(f\"\\n First few rows:\")\n",
    "        display(df_real_estate.head())\n",
    "    else:\n",
    "        print(\"No CSV files found in real_estate directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac87dc76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_real_estate' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA QUALITY ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n1. Missing Values:\")\n",
    "    missing = df_real_estate.isnull().sum()\n",
    "    missing_pct = (missing / len(df_real_estate)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing_Count': missing,\n",
    "        'Missing_Percentage': missing_pct\n",
    "    }).sort_values('Missing_Count', ascending=False)\n",
    "    \n",
    "    print(missing_df[missing_df['Missing_Count'] > 0])\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\n2. Data Types:\")\n",
    "    print(df_real_estate.dtypes)\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n3. Summary Statistics:\")\n",
    "    display(df_real_estate.describe())\n",
    "    \n",
    "    # Check for duplicates\n",
    "    duplicates = df_real_estate.duplicated().sum()\n",
    "    print(f\"\\n4. Duplicate Rows: {duplicates}\")\n",
    "    \n",
    "    # Date range (if Year column exists)\n",
    "    if 'Year' in df_real_estate.columns:\n",
    "        print(f\"\\n5. Year Range: {df_real_estate['Year'].min()} - {df_real_estate['Year'].max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5689d195",
   "metadata": {},
   "source": [
    "## 2. Real Estate Visualizations\n",
    "\n",
    "Let's visualize trends in the real estate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d698427a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_real_estate' in locals() and 'Year' in df_real_estate.columns:\n",
    "    # Check if we have price columns\n",
    "    price_cols = [col for col in df_real_estate.columns if 'price' in col.lower() or 'value' in col.lower()]\n",
    "    \n",
    "    if price_cols:\n",
    "        print(f\"Found price columns: {price_cols}\")\n",
    "        \n",
    "        # Create time series plot\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "        \n",
    "        # Plot 1: Average trends over time\n",
    "        if len(price_cols) > 0:\n",
    "            price_col = price_cols[0]\n",
    "            yearly_avg = df_real_estate.groupby('Year')[price_col].mean()\n",
    "            \n",
    "            axes[0].plot(yearly_avg.index, yearly_avg.values, marker='o', linewidth=2, markersize=8)\n",
    "            axes[0].set_title(f'Average {price_col} Over Time', fontsize=14, fontweight='bold')\n",
    "            axes[0].set_xlabel('Year', fontsize=12)\n",
    "            axes[0].set_ylabel(price_col, fontsize=12)\n",
    "            axes[0].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add trend line\n",
    "            z = np.polyfit(yearly_avg.index, yearly_avg.values, 1)\n",
    "            p = np.poly1d(z)\n",
    "            axes[0].plot(yearly_avg.index, p(yearly_avg.index), \"--\", alpha=0.5, color='red', label='Trend')\n",
    "            axes[0].legend()\n",
    "        \n",
    "        # Plot 2: Distribution of prices\n",
    "        if len(price_cols) > 0:\n",
    "            axes[1].hist(df_real_estate[price_col].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "            axes[1].set_title(f'Distribution of {price_col}', fontsize=14, fontweight='bold')\n",
    "            axes[1].set_xlabel(price_col, fontsize=12)\n",
    "            axes[1].set_ylabel('Frequency', fontsize=12)\n",
    "            axes[1].grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig(RESULTS / 'figures' / 'real_estate_trends.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nâœ“ Figure saved to: {RESULTS / 'figures' / 'real_estate_trends.png'}\")\n",
    "    else:\n",
    "        print(\"No price columns found for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf73ed0",
   "metadata": {},
   "source": [
    "## 3. Building Permits Data\n",
    "\n",
    "Let's load and explore development activity through building permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073be858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most recent building permits file\n",
    "permits_dir = DATA_RAW / 'permits'\n",
    "\n",
    "if not permits_dir.exists():\n",
    "    print(\"Building permits data not found. Run data collection first:\")\n",
    "    print(\"   python src/data_collection.py --sources permits\")\n",
    "else:\n",
    "    csv_files = list(permits_dir.glob('*.csv'))\n",
    "    \n",
    "    if csv_files:\n",
    "        latest_file = max(csv_files, key=lambda p: p.stat().st_mtime)\n",
    "        print(f\"âœ“ Loading: {latest_file.name}\")\n",
    "        \n",
    "        # Load with low_memory=False to avoid dtype warnings\n",
    "        df_permits = pd.read_csv(latest_file, low_memory=False)\n",
    "        \n",
    "        print(f\"\\n Dataset Shape: {df_permits.shape}\")\n",
    "        print(f\"   Rows: {df_permits.shape[0]:,}\")\n",
    "        print(f\"   Columns: {df_permits.shape[1]}\")\n",
    "        \n",
    "        print(f\"\\n Key Columns:\")\n",
    "        key_cols = ['ISSUED_DATE', 'PERMIT_TYPE', 'WORK_TYPE', 'LATITUDE', 'LONGITUDE', \n",
    "                    'ESTIMATED_VALUE', 'PROPOSED_BUILDING_TYPE']\n",
    "        for col in key_cols:\n",
    "            if col in df_permits.columns:\n",
    "                print(f\"   âœ“ {col}\")\n",
    "            else:\n",
    "                print(f\"   âœ— {col} (not found)\")\n",
    "        \n",
    "        print(f\"\\n First few rows:\")\n",
    "        display(df_permits.head())\n",
    "        \n",
    "        # Convert date column if exists\n",
    "        date_cols = [col for col in df_permits.columns if 'DATE' in col.upper()]\n",
    "        if date_cols:\n",
    "            for col in date_cols:\n",
    "                try:\n",
    "                    df_permits[col] = pd.to_datetime(df_permits[col], errors='coerce')\n",
    "                    print(f\"âœ“ Converted {col} to datetime\")\n",
    "                except:\n",
    "                    print(f\"âš  Could not convert {col} to datetime\")\n",
    "    else:\n",
    "        print(\" No CSV files found in permits directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7b42d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_permits' in locals():\n",
    "    print(\"=\" * 60)\n",
    "    print(\"BUILDING PERMITS ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Check for spatial data\n",
    "    has_coordinates = 'LATITUDE' in df_permits.columns and 'LONGITUDE' in df_permits.columns\n",
    "    \n",
    "    if has_coordinates:\n",
    "        valid_coords = df_permits[['LATITUDE', 'LONGITUDE']].notna().all(axis=1).sum()\n",
    "        print(f\"\\n1. Spatial Coverage:\")\n",
    "        print(f\"   Records with valid coordinates: {valid_coords:,} ({valid_coords/len(df_permits)*100:.1f}%)\")\n",
    "    \n",
    "    # Permit types\n",
    "    if 'PERMIT_TYPE' in df_permits.columns:\n",
    "        print(f\"\\n2. Permit Types:\")\n",
    "        permit_counts = df_permits['PERMIT_TYPE'].value_counts()\n",
    "        print(permit_counts.head(10))\n",
    "    \n",
    "    # Temporal distribution\n",
    "    if date_cols:\n",
    "        date_col = date_cols[0]\n",
    "        df_permits['Year'] = df_permits[date_col].dt.year\n",
    "        print(f\"\\n3. Temporal Distribution:\")\n",
    "        print(f\"   Date range: {df_permits[date_col].min()} to {df_permits[date_col].max()}\")\n",
    "        print(f\"\\n   Permits by year:\")\n",
    "        print(df_permits['Year'].value_counts().sort_index())\n",
    "    \n",
    "    # Estimated values\n",
    "    if 'ESTIMATED_VALUE' in df_permits.columns:\n",
    "        print(f\"\\n4. Construction Values:\")\n",
    "        # Convert to numeric if needed\n",
    "        df_permits['ESTIMATED_VALUE_NUM'] = pd.to_numeric(\n",
    "            df_permits['ESTIMATED_VALUE'], errors='coerce'\n",
    "        )\n",
    "        \n",
    "        values_summary = df_permits['ESTIMATED_VALUE_NUM'].describe()\n",
    "        print(values_summary)\n",
    "        \n",
    "        total_value = df_permits['ESTIMATED_VALUE_NUM'].sum()\n",
    "        print(f\"\\n   Total estimated construction value: ${total_value:,.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a646ebc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_permits' in locals() and 'Year' in df_permits.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # Plot 1: Permits over time\n",
    "    yearly_permits = df_permits['Year'].value_counts().sort_index()\n",
    "    axes[0, 0].bar(yearly_permits.index, yearly_permits.values, color='steelblue', alpha=0.7)\n",
    "    axes[0, 0].set_title('Building Permits Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0, 0].set_xlabel('Year')\n",
    "    axes[0, 0].set_ylabel('Number of Permits')\n",
    "    axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Plot 2: Permit types\n",
    "    if 'PERMIT_TYPE' in df_permits.columns:\n",
    "        top_types = df_permits['PERMIT_TYPE'].value_counts().head(10)\n",
    "        axes[0, 1].barh(range(len(top_types)), top_types.values, color='coral', alpha=0.7)\n",
    "        axes[0, 1].set_yticks(range(len(top_types)))\n",
    "        axes[0, 1].set_yticklabels(top_types.index, fontsize=9)\n",
    "        axes[0, 1].set_title('Top 10 Permit Types', fontsize=14, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Count')\n",
    "        axes[0, 1].grid(True, alpha=0.3, axis='x')\n",
    "    \n",
    "    # Plot 3: Construction values over time (if available)\n",
    "    if 'ESTIMATED_VALUE_NUM' in df_permits.columns:\n",
    "        yearly_value = df_permits.groupby('Year')['ESTIMATED_VALUE_NUM'].sum() / 1e9  # Convert to billions\n",
    "        axes[1, 0].plot(yearly_value.index, yearly_value.values, marker='o', linewidth=2, color='green')\n",
    "        axes[1, 0].set_title('Total Construction Value Over Time', fontsize=14, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Year')\n",
    "        axes[1, 0].set_ylabel('Value (Billions $)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot 4: Value distribution\n",
    "    if 'ESTIMATED_VALUE_NUM' in df_permits.columns:\n",
    "        # Filter out extreme outliers for better visualization\n",
    "        values = df_permits['ESTIMATED_VALUE_NUM'].dropna()\n",
    "        q99 = values.quantile(0.99)\n",
    "        values_filtered = values[values <= q99]\n",
    "        \n",
    "        axes[1, 1].hist(values_filtered, bins=50, color='purple', alpha=0.7, edgecolor='black')\n",
    "        axes[1, 1].set_title('Distribution of Construction Values (99th percentile)', fontsize=14, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Estimated Value ($)')\n",
    "        axes[1, 1].set_ylabel('Frequency')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(RESULTS / 'figures' / 'building_permits_analysis.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n Figure saved to: {RESULTS / 'figures' / 'building_permits_analysis.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc0c298",
   "metadata": {},
   "source": [
    "## 4. Spatial Visualization - Building Permits Map\n",
    "\n",
    "Let's create an interactive map showing the geographic distribution of building permits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913d2b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'df_permits' in locals() and has_coordinates:\n",
    "    print(\"Creating interactive map of building permits...\")\n",
    "    \n",
    "    # Filter to valid coordinates within GTA bounds\n",
    "    # GTA approximate bounds: Lat 43.5-44.0, Lon -79.8 to -79.0\n",
    "    df_map = df_permits[\n",
    "        (df_permits['LATITUDE'].notna()) & \n",
    "        (df_permits['LONGITUDE'].notna()) &\n",
    "        (df_permits['LATITUDE'] > 43.5) & \n",
    "        (df_permits['LATITUDE'] < 44.0) &\n",
    "        (df_permits['LONGITUDE'] > -79.8) & \n",
    "        (df_permits['LONGITUDE'] < -79.0)\n",
    "    ].copy()\n",
    "    \n",
    "    print(f\" Filtered to {len(df_map):,} permits within GTA bounds\")\n",
    "    \n",
    "    if len(df_map) > 0:\n",
    "        # For performance, sample if dataset is too large\n",
    "        if len(df_map) > 5000:\n",
    "            print(f\" Sampling 5,000 permits for map performance\")\n",
    "            df_map = df_map.sample(n=5000, random_state=42)\n",
    "        \n",
    "        # Create base map centered on Toronto\n",
    "        toronto_center = [43.7, -79.4]\n",
    "        m = folium.Map(\n",
    "            location=toronto_center,\n",
    "            zoom_start=11,\n",
    "            tiles='OpenStreetMap'\n",
    "        )\n",
    "        \n",
    "        # Add marker cluster for better performance\n",
    "        from folium.plugins import MarkerCluster\n",
    "        marker_cluster = MarkerCluster(name='Building Permits').add_to(m)\n",
    "        \n",
    "        # Add markers\n",
    "        for idx, row in df_map.iterrows():\n",
    "            # Create popup text\n",
    "            popup_text = f\"\"\"\n",
    "            <b>Permit Type:</b> {row.get('PERMIT_TYPE', 'N/A')}<br>\n",
    "            <b>Date:</b> {row.get('ISSUED_DATE', 'N/A')}<br>\n",
    "            <b>Value:</b> ${row.get('ESTIMATED_VALUE', 'N/A'):,} <br>\n",
    "            <b>Building Type:</b> {row.get('PROPOSED_BUILDING_TYPE', 'N/A')}\n",
    "            \"\"\"\n",
    "            \n",
    "            folium.Marker(\n",
    "                location=[row['LATITUDE'], row['LONGITUDE']],\n",
    "                popup=folium.Popup(popup_text, max_width=300),\n",
    "                icon=folium.Icon(color='blue', icon='info-sign')\n",
    "            ).add_to(marker_cluster)\n",
    "        \n",
    "        # Add layer control\n",
    "        folium.LayerControl().add_to(m)\n",
    "        \n",
    "        # Save map\n",
    "        map_file = RESULTS / 'figures' / 'permits_map.html'\n",
    "        m.save(str(map_file))\n",
    "        \n",
    "        print(f\"\\n Interactive map saved to: {map_file}\")\n",
    "        print(f\" Open in browser to view: file://{map_file.absolute()}\")\n",
    "        \n",
    "        # Display in notebook\n",
    "        display(m)\n",
    "    else:\n",
    "        print(\" No valid coordinates found for mapping\")\n",
    "else:\n",
    "    print(\" Building permits data or coordinates not available for mapping\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687a57f9",
   "metadata": {},
   "source": [
    "## 5. Data Summary and Export\n",
    "\n",
    "Let's create summary tables for the midterm report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab2e00be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive summary table\n",
    "summary_data = []\n",
    "\n",
    "# Real Estate Summary\n",
    "if 'df_real_estate' in locals():\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Real Estate / Housing',\n",
    "        'Records': f\"{len(df_real_estate):,}\",\n",
    "        'Date Range': f\"{df_real_estate['Year'].min()}-{df_real_estate['Year'].max()}\" if 'Year' in df_real_estate.columns else 'N/A',\n",
    "        'Spatial Granularity': 'FSA / Neighbourhood',\n",
    "        'Status': ' Complete',\n",
    "        'Notes': f\"{df_real_estate.shape[1]} columns\"\n",
    "    })\n",
    "\n",
    "# Building Permits Summary\n",
    "if 'df_permits' in locals():\n",
    "    date_range = 'N/A'\n",
    "    if 'Year' in df_permits.columns:\n",
    "        date_range = f\"{df_permits['Year'].min()}-{df_permits['Year'].max()}\"\n",
    "    \n",
    "    summary_data.append({\n",
    "        'Data Source': 'Building Permits',\n",
    "        'Records': f\"{len(df_permits):,}\",\n",
    "        'Date Range': date_range,\n",
    "        'Spatial Granularity': 'Lat/Lon coordinates',\n",
    "        'Status': ' Complete',\n",
    "        'Notes': f\"{valid_coords:,} with valid coordinates\"\n",
    "    })\n",
    "\n",
    "# Demographics (placeholder)\n",
    "demographics_dir = DATA_RAW / 'demographics'\n",
    "if demographics_dir.exists() and list(demographics_dir.glob('*.csv')):\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Demographics (Census)',\n",
    "        'Records': 'TBD',\n",
    "        'Date Range': '2021',\n",
    "        'Spatial Granularity': 'DA / Neighbourhood',\n",
    "        'Status': ' In Progress',\n",
    "        'Notes': 'Data collected, needs processing'\n",
    "    })\n",
    "else:\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Demographics (Census)',\n",
    "        'Records': 'N/A',\n",
    "        'Date Range': 'N/A',\n",
    "        'Spatial Granularity': 'DA / Neighbourhood',\n",
    "        'Status': ' Not Started',\n",
    "        'Notes': 'Pending collection'\n",
    "    })\n",
    "\n",
    "# Transit Networks (placeholder)\n",
    "transit_dir = DATA_RAW / 'transit'\n",
    "if transit_dir.exists() and list(transit_dir.glob('*.graphml')):\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Road Network (OSM)',\n",
    "        'Records': 'Network Graph',\n",
    "        'Date Range': '2024',\n",
    "        'Spatial Granularity': 'Node/Edge level',\n",
    "        'Status': ' In Progress',\n",
    "        'Notes': 'Network downloaded'\n",
    "    })\n",
    "else:\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Road Network (OSM)',\n",
    "        'Records': 'N/A',\n",
    "        'Date Range': 'N/A',\n",
    "        'Spatial Granularity': 'Node/Edge level',\n",
    "        'Status': ' Not Started',\n",
    "        'Notes': 'Pending collection'\n",
    "    })\n",
    "\n",
    "# Amenities/POIs (placeholder)\n",
    "amenities_dir = DATA_RAW / 'amenities'\n",
    "if amenities_dir.exists() and list(amenities_dir.glob('*.geojson')):\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Amenities/POIs (OSM)',\n",
    "        'Records': 'TBD',\n",
    "        'Date Range': '2024',\n",
    "        'Spatial Granularity': 'Point locations',\n",
    "        'Status': ' In Progress',\n",
    "        'Notes': 'Data collected, needs processing'\n",
    "    })\n",
    "else:\n",
    "    summary_data.append({\n",
    "        'Data Source': 'Amenities/POIs (OSM)',\n",
    "        'Records': 'N/A',\n",
    "        'Date Range': 'N/A',\n",
    "        'Spatial Granularity': 'Point locations',\n",
    "        'Status': ' Not Started',\n",
    "        'Notes': 'Pending collection'\n",
    "    })\n",
    "\n",
    "# Create DataFrame\n",
    "df_summary = pd.DataFrame(summary_data)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA COLLECTION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "display(df_summary)\n",
    "\n",
    "# Save to CSV for report\n",
    "summary_file = RESULTS / 'tables' / 'data_summary.csv'\n",
    "df_summary.to_csv(summary_file, index=False)\n",
    "print(f\"\\nâœ“ Summary table saved to: {summary_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf29a66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create statistics dictionary for report\n",
    "stats = {\n",
    "    'collection_date': datetime.now().strftime('%Y-%m-%d'),\n",
    "    'data_sources_collected': len([s for s in summary_data if s['Status'] == ' Complete']),\n",
    "    'total_data_sources': len(summary_data),\n",
    "}\n",
    "\n",
    "if 'df_real_estate' in locals():\n",
    "    stats['real_estate_records'] = len(df_real_estate)\n",
    "    if 'Year' in df_real_estate.columns:\n",
    "        stats['real_estate_years'] = f\"{df_real_estate['Year'].min()}-{df_real_estate['Year'].max()}\"\n",
    "\n",
    "if 'df_permits' in locals():\n",
    "    stats['building_permits_records'] = len(df_permits)\n",
    "    stats['permits_with_coordinates'] = valid_coords\n",
    "    if 'ESTIMATED_VALUE_NUM' in df_permits.columns:\n",
    "        stats['total_construction_value_billions'] = round(df_permits['ESTIMATED_VALUE_NUM'].sum() / 1e9, 2)\n",
    "\n",
    "# Save statistics\n",
    "import json\n",
    "stats_file = RESULTS / 'tables' / 'collection_stats.json'\n",
    "with open(stats_file, 'w') as f:\n",
    "    json.dump(stats, f, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"KEY STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key:40s}: {value}\")\n",
    "\n",
    "print(f\"\\nâœ“ Statistics saved to: {stats_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694244d5",
   "metadata": {},
   "source": [
    "## 6. Next Steps\n",
    "\n",
    "### Completed \n",
    "- Data collection from Toronto Open Data (real estate, building permits)\n",
    "- Initial data quality assessment\n",
    "- Summary statistics and visualizations\n",
    "- Interactive spatial maps\n",
    "\n",
    "### To Do ðŸ”œ\n",
    "1. **Week 2-3 (Network Construction)**\n",
    "   - Aggregate data to FSA level\n",
    "   - Build spatial network with adjacency/distance edges\n",
    "   - Calculate travel times using road network\n",
    "   - Compute centrality measures\n",
    "\n",
    "2. **Week 2-3 (Feature Engineering)**\n",
    "   - Accessibility features (distance to downtown, transit)\n",
    "   - Amenity density (schools, parks, commercial)\n",
    "   - Development activity (permit counts and values)\n",
    "   - Spatial lag features (neighborhood effects)\n",
    "   - Temporal features (historical growth rates)\n",
    "\n",
    "3. **Week 4-5 (Baseline Models)**\n",
    "   - Naive persistence baseline\n",
    "   - LASSO regression with feature selection\n",
    "   - XGBoost gradient boosting\n",
    "\n",
    "4. **Week 6+ (Spatial Models)**\n",
    "   - Spatial Autoregressive (SAR) model\n",
    "   - Geographically Weighted Regression (GWR)\n",
    "   - Graph Convolutional Network (GCN) if time permits\n",
    "\n",
    "### For Midterm Report\n",
    "- Include data summary table from Cell 15\n",
    "- Include key visualizations (Cells 7, 11)\n",
    "- Document data quality issues and mitigation strategies\n",
    "- Outline feature engineering plan"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
